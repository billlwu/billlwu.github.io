---
title: Lec1
---

Introduction of the RL paradigm. Its main distinction from ML is *sequential decision making under uncertainty*, where the goal is for the agent to select actions to maximize total future reward. Here is a schematic and a couple of real-world applications:

![picture](/RL/RL_prob.png)

This feedback loop can be recorded as a stream of observations, rewards, actions, known as the history: ![equation](https://latex.codecogs.com/gif.latex?H_t%20%3D%20O_1%2C%20R_1%2C%20A_1%2C%20...%2C%20O_%7Bt-1%7D%2C%20R_%7Bt-1%7D%2C%20A_%7Bt-1%7D%2C%20O_t%2C%20R_t). State is a concise summary of history, and is an entity that contains the information used to determine what happens next: ![equation](https://latex.codecogs.com/gif.latex?S_t%20%3D%20f%28H_t%29). There are 3 states in the RL problem: environment ![equation](https://latex.codecogs.com/gif.latex?S_t%5Ee), agent![equation](https://latex.codecogs.com/gif.latex?S_t%5Ea), and information state (a.k.a. Markov state).

This course will be dealing with fully observable environments, where the 3 states are equal ![equation](https://latex.codecogs.com/gif.latex?O_t%3DS_t%5Ea%3DS_t%5Ee). Formally, this is a Markov decision process. The MDP (a mathematical framework) describes the class of problems RL aims to solve. When the agent only indirectly observes the environment (i.e. when a trader only sees current prices), then ![equation](https://latex.codecogs.com/gif.latex?S_t%5Ea%5Cneq%20S_t%5Ee) and we have a *partially observable Markov decision process* (POMDP). The agent will need to construct its own state representation. 

The major components of an RL agent includes (but not limited to): policy, value function, model. Policy is the agent's behavior, and can be deterministic or stochastic (![equation](https://latex.codecogs.com/gif.latex?A_t%3D%5Cpi%28S_t%29) or ![equation](https://latex.codecogs.com/gif.latex?%5Cpi%28a%7Cs%29%3DP%28A%3Da%7CS%3Ds%29)). The value function is a prediction of future reward ![equation](https://latex.codecogs.com/gif.latex?v_%5Cpi%28s%29%3DE_%5Cpi%5BR_%7Bt&plus;1%7D&plus;%5Cgamma%20R_%7Bt&plus;2%7D&plus;...%7CS_t%3Ds%5D). The model predicts what the environment will do next (the next state and reward given the current state and action: ![equation](https://latex.codecogs.com/gif.latex?P%28S_%7Bt&plus;1%7D%3Ds%27%2C%20R_%7Bt&plus;1%7D%3Dr%7CS_t%3Ds%2C%20A_t%3Da%29)). Agent has to simultaneously learn model and solve for the optimal policy. 

I forked CME241's code base. To sync with the upstream repo, type `git fetch upstream` and then `git merge upstream/master`.
