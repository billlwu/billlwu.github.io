---
title: Lec1
---

L1 introduces RL paradigm, and its main distinction from ML is *sequential decision making*, where the goal is for the agent to select actions to maximize total future reward. Here is an illustration of the feedback loop between an agent and its environment:

![picture](/RL/RL_problem.png)

This feedback loop can be recorded as a stream of observations, rewards, actions, known as the history: ![equation](https://latex.codecogs.com/gif.latex?H_t%20%3D%20O_1%2C%20R_1%2C%20A_1%2C%20...%2C%20O_%7Bt-1%7D%2C%20R_%7Bt-1%7D%2C%20A_%7Bt-1%7D%2C%20O_t%2C%20R_t). State is a concise summary of history, and is the information used to determine what happens next: ![equation](https://latex.codecogs.com/gif.latex?S_t%20%3D%20f%28H_t%29). There are 3 states in the RL problem: environment ![equation](https://latex.codecogs.com/gif.latex?S_t%5Ee), agent![equation](https://latex.codecogs.com/gif.latex?S_t%5Ea), and information state (a.k.a. Markov state).

This course will be dealing with fully observable environments, where the 3 states are equal ![equation](https://latex.codecogs.com/gif.latex?O_t%3DS_t%5Ea%3DS_t%5Ee). Formally, this is a Markov decision process (MDP). When the agent only indirectly observes the environment (i.e. when a trader only sees current prices), then ![equation](https://latex.codecogs.com/gif.latex?S_t%5Ea%5Cneq%20S_t%5Ee) and we have a *partially observable Markov decision process*, or POMDP. The agent will need to construct its own state representation. 

The major components of an RL agent includes (but not limited to): policy, value function, model. Policy is the agent's behavior, deterministic or stochastic (![equation](https://latex.codecogs.com/gif.latex?A_t%3D%5Cpi%28S_t%29) or ![equation](https://latex.codecogs.com/gif.latex?%5Cpi%28a%7Cs%29%3DP%28A%3Da%7CS%3Ds%29)). Value function is a prediction of future reward ![equation](https://latex.codecogs.com/gif.latex?v_%5Cpi%28s%29%3DE_%5Cpi%5BR_%7Bt&plus;1%7D&plus;%5Cgamma%20R_%7Bt&plus;2%7D&plus;...%7CS_t%3Ds%5D). The model predicts what the environment will do next (the next state and reward given the current state and action). 
