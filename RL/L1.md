---
title: Lec1
---

Introduction of the RL paradigm, and its main distinction from ML is *sequential decision making under uncertainty*, where the goal is for the agent to select actions to maximize total future reward. Here is a schematic and a couple of real-world applications of the RL paradigm:

![picture](/RL/RL_prob.png)

This feedback loop can be recorded as a stream of observations, rewards, actions, known as the history: ![equation](https://latex.codecogs.com/gif.latex?H_t%20%3D%20O_1%2C%20R_1%2C%20A_1%2C%20...%2C%20O_%7Bt-1%7D%2C%20R_%7Bt-1%7D%2C%20A_%7Bt-1%7D%2C%20O_t%2C%20R_t). State is a concise summary of history, and is an entity that contains the information used to determine what happens next: ![equation](https://latex.codecogs.com/gif.latex?S_t%20%3D%20f%28H_t%29). There are 3 states in the RL problem: environment ![equation](https://latex.codecogs.com/gif.latex?S_t%5Ee), agent![equation](https://latex.codecogs.com/gif.latex?S_t%5Ea), and information state (a.k.a. Markov state).

This course will be dealing with fully observable environments, where the 3 states are equal ![equation](https://latex.codecogs.com/gif.latex?O_t%3DS_t%5Ea%3DS_t%5Ee). Formally, this is a Markov decision process (MDP). MDP (a mathematical framework) describes the class of problems RL aims to solve. When the agent only indirectly observes the environment (i.e. when a trader only sees current prices), then ![equation](https://latex.codecogs.com/gif.latex?S_t%5Ea%5Cneq%20S_t%5Ee) and we have a *partially observable Markov decision process*, or POMDP. The agent will need to construct its own state representation. 

The major components of an RL agent includes (but not limited to): policy, value function, model. Policy is the agent's behavior, and can be deterministic or stochastic (![equation](https://latex.codecogs.com/gif.latex?A_t%3D%5Cpi%28S_t%29) or ![equation](https://latex.codecogs.com/gif.latex?%5Cpi%28a%7Cs%29%3DP%28A%3Da%7CS%3Ds%29)). Value function is a prediction of future reward ![equation](https://latex.codecogs.com/gif.latex?v_%5Cpi%28s%29%3DE_%5Cpi%5BR_%7Bt&plus;1%7D&plus;%5Cgamma%20R_%7Bt&plus;2%7D&plus;...%7CS_t%3Ds%5D). The model predicts what the environment will do next (the next state and reward given the current state and action). 

The agent's job is to exercise control (with goal of max. value function) by determining the optimal policy function. Hence, this control is dynamic (time-sequenced). 
